{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/VitcSS/Word_Embedding/blob/master/deliver.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.test.utils import datapath\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "try :\n",
    "    import umap\n",
    "except:\n",
    "    !pip install umap\n",
    "finally:\n",
    "    import umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_word2vec_model(dataset, model_save_path):\n",
    "    # Download the dataset\n",
    "    corpus = api.load(dataset)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(model_save_path)\n",
    "\n",
    "def download_text8():\n",
    "    # Download the Text8 dataset\n",
    "    return api.load(\"text8\")\n",
    "\n",
    "def download_wikipedia_dump():\n",
    "    # Download the Wikipedia dump\n",
    "    # Note: This is a large file, and the download may take some time\n",
    "    return api.load(\"enwiki-latest-pages-articles\")\n",
    "\n",
    "def train_word2vec_model(corpus, model_save_path):\n",
    "    # Tokenize the text\n",
    "    tokenized_text = [str(sentence).lower().split() for sentence in corpus]\n",
    "\n",
    "    # Train the Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(model_save_path)\n",
    "\n",
    "def evaluate_word_similarity(model_path, similarity_dataset):\n",
    "    # Load the trained model\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "    # Evaluate on Word Similarity Task\n",
    "    similarities = model.wv.evaluate_word_pairs(datapath(similarity_dataset))\n",
    "    print(\"Spearman correlation for Word Similarity Task:\", similarities[0])\n",
    "\n",
    "def evaluate_word_analogy(model_path, analogy_dataset):\n",
    "    # Load the trained model\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "    # Evaluate on Word Analogy Task\n",
    "    analogies = model.wv.evaluate_word_analogies(datapath(analogy_dataset))\n",
    "    print(\"Accuracy on Word Analogy Task:\", analogies[0])\n",
    "\n",
    "def visualize_word_embeddings(model_path):\n",
    "    # Load the trained Word2Vec model\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "    # Get word vectors and corresponding words\n",
    "    words = list(model.wv.index_to_key)\n",
    "    word_vectors = [model.wv[word] for word in words]\n",
    "\n",
    "    # Dimensionality reduction using UMAP\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(word_vectors)\n",
    "\n",
    "    # Scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embedding[:, 0], embedding[:, 1], marker=\".\", s=5, alpha=0.5)\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(embedding[i, 0], embedding[i, 1]), fontsize=8)\n",
    "    plt.title(\"UMAP Visualization of Word Embeddings\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Download and process the Text8 dataset\n",
    "text8_corpus = download_text8()\n",
    "\n",
    "# Train and evaluate on Text8 dataset\n",
    "train_word2vec_model(text8_corpus, \"word2vec_model_text8.model\")\n",
    "evaluate_word_similarity(\"word2vec_model_text8.model\", \"wordsim353.tsv\")\n",
    "evaluate_word_analogy(\"word2vec_model_text8.model\", \"questions-words.txt\")\n",
    "\n",
    "# Download and process the Wikipedia dump\n",
    "wikipedia_corpus = download_wikipedia_dump()\n",
    "\n",
    "# Train and evaluate on Wikipedia dataset\n",
    "train_word2vec_model(wikipedia_corpus, \"word2vec_model_wikipedia.model\")\n",
    "evaluate_word_similarity(\"word2vec_model_wikipedia.model\", \"wordsim353.tsv\")\n",
    "evaluate_word_analogy(\"word2vec_model_wikipedia.model\", \"questions-words.txt\")\n",
    "\n",
    "# Visualize word embeddings\n",
    "visualize_word_embeddings(\"word2vec_model_wikipedia.model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WE_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
